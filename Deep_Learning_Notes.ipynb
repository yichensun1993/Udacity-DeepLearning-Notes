{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Windows\n",
    "Install Docker\n",
    "Download and install Docker from the official Docker website.\n",
    "\n",
    "Run the Docker Container\n",
    "Run the command below to start a jupyter notebook server with TensorFlow:\n",
    "\n",
    "**docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow**\n",
    "Users in China should use the b.gcr.io/tensorflow/tensorflow instead of gcr.io/tensorflow/tensorflow\n",
    "\n",
    "You can access the jupyter notebook at localhost:8888. The server includes 3 examples of TensorFlow notebooks,\n",
    "but you can create a new notebook to test all your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tensor\n",
    "In TensorFlow, data isnâ€™t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. \n",
    "\n",
    "In the case of hello_constant = tf.constant('Hello World!'), hello_constant is a 0-dimensional string tensor.\n",
    "\n",
    "The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes.\n",
    "\n",
    "![Flowchart](https://d17h27t6h515a5.cloudfront.net/topher/2016/October/580feadb_session/session.png \"TensorFlow Sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.6699981689\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string) ##The tensor x being set to the string \"Hello, world\"\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z, feed_dict={x: 'Test String', y: 123, z: 45.67}) ##Feed x,y,z tensor values\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.add(5, 2)  # 7\n",
    "y = tf.subtract(10, 4) # 6\n",
    "z = tf.multiply(2, 5)  # 10\n",
    "w = tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1\n",
    "## Cast a value to another type. In this case, converting the 2.0 to an integer before subtracting\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1), tf.float64))## Data type is float64 after division\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow Linear Function\n",
    "For example, if we want to classify images as digits.\n",
    "\n",
    "x would be our list of pixel values, and y would be the logits, one for each digit. Let's take a look at y = Wx, where the weights, W, determine the influence of x at predicting each y.\n",
    "\n",
    "![Classification](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5839dd1c_wx-1/wx-1.jpg \"classification of characters\")\n",
    "\n",
    "In TensorFlow, we actually use y = xW + b, because this is what TensorFlow uses.\n",
    "\n",
    "![TensorFlow Forms](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58353057_codecogseqn-18/codecogseqn-18.gif)\n",
    "\n",
    "\n",
    "# Weights and Bias in TensorFlow\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can't be modified. This is where **tf.Variable** class comes in.\n",
    "\n",
    "** x = tf.Variable(5) **\n",
    "\n",
    "The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors.\n",
    "\n",
    "**init = tf.global_variables_initializer()**  \n",
    "** with tf.Session() as sess: **  \n",
    "**    sess.run(init) **\n",
    "    \n",
    "The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the **tf.truncated_normal()** function to generate random numbers from a normal distribution.\n",
    "\n",
    "**n_features = 120**  \n",
    "**n_labels = 5**  \n",
    "**weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))**  \n",
    "\n",
    "The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use **tf.zeros()** to set the bias to 0.\n",
    "\n",
    "**n_labels = 5**  \n",
    "**bias = tf.Variable(tf.zeros(n_labels))**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Linear Classifier Quiz\n",
    "![Subset of MNIST dataset](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/582cf7a7_mnist-012/mnist-012.png)\n",
    "\n",
    "You'll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. The above is a small sample of the data you'll be training on. Notice how some of the 1s are written with a serif at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model.\n",
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/582ce9ef_weights-0-1-2/weights-0-1-2.png\" width=\"500\">\n",
    "\n",
    "The images above are trained weights for each label (0, 1, and 2). The weights display the unique properties of each digit they have found. Complete this quiz to train your own weights using the MNIST dataset.\n",
    "\n",
    "### Instructions\n",
    "1. Open quiz.py.\n",
    "    1. Implement get_weights to return a tf.Variable of weights\n",
    "    2. Implement get_biases to return a tf.Variable of biases\n",
    "    3. Implement xW + b in the linear function\n",
    "2. Open sandbox.py\n",
    "    1. Initialize all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Loss: 8.50671863556\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b) ## xW in xW + b is matrix multiplication\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We combine two files sandbox.py and quiz.py(Defined functions) together.  \n",
    "We see in the sandbox.py sextion, we could import training data from tensorflow.examples.tutorials.mnist.\n",
    "Further readings about [MNIST](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/)\n",
    "\n",
    "# Softmax\n",
    "The next step is to assign a probability to each label, which you can then use to classify the data. Use the softmax function to turn your logits into probabilities.\n",
    "\n",
    "$S(y_i)=\\dfrac{e^{y_i}}{\\sum_j e^{y_j}}$\n",
    "\n",
    "For the next quiz, you'll implement a softmax(x) function that takes in x, a one or two dimensional array of logits.\n",
    "\n",
    "In the one dimensional case, the array is just a single set of logits. In the two dimensional case, each column in the array is a set of logits. The softmax(x) function should return a NumPy array of the same shape as x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
      " [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
      " [ 0.66524096  0.97962921  0.86681333  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "## In a two-dimensional array, each column represents a set of logits:\n",
    "logits = np.array([\n",
    "    [1, 2, 3, 6],\n",
    "    [2, 4, 5, 6],\n",
    "    [3, 8, 7, 6]])\n",
    "\n",
    "print(softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's see how softmax is done in TensorFlow:\n",
    "\n",
    "**x = tf.nn.softmax([2.0, 1.0, 0.2])**  \n",
    "\n",
    "tf.nn.softmax() implements the softmax function for you. It takes in logits and returns softmax activations.\n",
    "\n",
    "The following cell use the softmax function to return the softmax of the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When the logits are multiplied by 10, the probabilities get close to 0.0 or 1.0 while when the logits are divided by 10,\n",
    "the probabilities get close to the uniform distribution.(Less distinguished) In other words, when I increase the size of the outputs, my classifier would become very confident about its predictions. And for our neural network, we want it not to be very confident about its predictions at the beginning, but as it learns over time, it shall gain more confidence.\n",
    "\n",
    "**One Hot Encoding** to represent different labels mathematically. A predicted output would be a vector of the size \"1 * feature\", where each column is the possibility corresponding to each class/label. And by one hot encoding, each label is represented by a vector with one 1 and the rest 0, i.e. [0 0 1 0 0]. \n",
    "\n",
    "When there are so many classes that one hot encoding would become inefficient as it has 0 almost everywhere, we will solve this problem with embeddings later. \n",
    "\n",
    "Now, we could measure how well we are doing predictions by simply comparing the vector that comes out of classifiers and contains the probabilities of your classes and the one hot encoded vector that corresponds to our labels.\n",
    "\n",
    "# Cross Entropy\n",
    "\n",
    "\n",
    "$D(S,L)=-\\sum_i L_i \\log(S_i)$,  S for the output prediction and L for the Label.\n",
    "\n",
    "Remember to have the labels and the distributions in the right place because the function is non-commutatuive, $D(S,L)\\neq D(L,S)$.\n",
    "\n",
    "<img src=\"https://s29.postimg.org/b6oxz186v/flowchart.png\" width=\"500\">\n",
    "\n",
    "Above is usually called **multinomial logistic classification**\n",
    "\n",
    "### Reduce Sum\n",
    "\n",
    "**x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15**\n",
    "\n",
    "The tf.reduce_sum() function takes an array of numbers and sums them together.\n",
    "\n",
    "### Natural Log\n",
    "\n",
    "**x = tf.log(100)  # 4.60517**\n",
    "\n",
    "This function does exactly what you would expect it to do. tf.log() takes the natural log of a number.\n",
    "\n",
    "We implement these two functions to calculate the cross entropy as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot,tf.log(softmax)))\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy,feed_dict={softmax: softmax_data, one_hot: one_hot_data})\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are calculating D(S(wx + b),L) for our input and we want the cross entropy, D, to have a high distance for the incorrect class D(A,b) while a low distance for the correct class D(A,a). \n",
    "\n",
    "### Training Loss\n",
    "\n",
    "$L = \\dfrac{1}{N}\\sum D(S(wx_i+b),L_i)$ \n",
    "\n",
    "to measure the distance averaged over the entire training set for all the inputs and all the labels available.\n",
    "\n",
    "We want this loss, which characterize how well we are classifying every example in the training data, to be small. And as a function of weights and biases, we could use gradient descent to minimize this numerical funciton.\n",
    "\n",
    "We are now to discuss the tools that compute these derivatives, and the good and bad about gradient descent. Besides, there are still two major problems we need to solve before training our own model:\n",
    "1. How to dill image pixels to the classifier\n",
    "2. Where to initialize the optimization\n",
    "\n",
    "# Numerical Stability\n",
    "We now add $10^{-6}$ $10^6$ times to $10^9$ and subtract $10^9$ again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.953674316406\n"
     ]
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When we are calculating values that are too large or too small, especially when we are adding very small values to a very large value, a lot of errors could be introduced. Alternatively, when we replace the 1,000,000,000 with 1 in the example above, we would see the error to be very tiny. \n",
    "\n",
    "In this case, we want our training loss function never to be too big or too small. In practice, a good guiding principle is that we always want our variables to have zero mean and equal variance whenever possible.(Imagine an oval away from the origin and a circle with its center right at the origin. Our classfier doesn't need to do a lot of searching to find a good solution when the circle is around the origin.)\n",
    "\n",
    "To deal with images, each pixel's RGB value, [0,255] could be processed in the following way to make it much easier for the optimization to proceed numerically:\n",
    "\n",
    "$Pixel = [\\dfrac{R-128}{128}, \\dfrac{G-128}{128}, \\dfrac{B-128}{128}]$\n",
    "\n",
    "In this way, the training data is normalized to have zero mean and unit variance.\n",
    "\n",
    "To have weights and biases initialized at a good starting point for the gradient descent to proceed, we could draw the weights randomly from a Guassian distribution with zero mean and standard deviation $\\sigma$. The $\\sigma$ determines the order of magnitude of outputs at the initial point of optimization. \n",
    "\n",
    "As we have illustrated previously in the softmax section, due to the softmax function on top of it, i.e. S(wx+b), the order of magnitudes also determines the peakness of the initial probability distribution. A large $\\sigma$ means large peaks(more distinguished distributed possibility in each output), which means the classifier would be very opinionated, which is not what we want at the beginning. As we said, we want it not to be very confident about its predictions at the beginning, but as it learns over time, it shall gain more confidence. So we shall use a small $\\sigma$ to begin with.\n",
    "\n",
    "With all what we have now, ou magical packages, at last, would calculate for us:\n",
    "1. $w-\\alpha \\Delta_wL$\n",
    "2. $b-\\alpha \\Delta_bL$\n",
    "\n",
    "The derivative of loss with respect to weights and biases and takes a step back in the direction opposite to that derivative.\n",
    "\n",
    "Another important feature about our classifiers is that they tend to perform well on images within our training set but fails to do as well on new images. This is because it has memorized the training set and fails to generalize to new examples. In fact, every classifier that you will build will tend to try and memorize the training sets, but we will help it generalize to new data instead. So, how do we measure how well the classifier generalizes rather than simply memorizing data?\n",
    "\n",
    "One way to measure our classifier performance is by hiding a portion of training data and only use them for evaluation after the classifier is properly tuned. However, training a classifier is a process of trials and errors, so when we choose what classifier to use, or how parameters are modified based on our test set, we are making modifications based on our observation of the performance, and though little information is fed into the classifier with our tuning, it adds up. So now the test data bleeds into our training data.\n",
    "\n",
    "# Cross Validation\n",
    "\n",
    "There are some information about cross validation which I found on websites:\n",
    "\n",
    "[Why every statistician should know about cross-validation](http://robjhyndman.com/hyndsight/crossvalidation/)\n",
    "\n",
    "[TAMU Lecture: Validation](http://research.cs.tamu.edu/prism/lectures/iss/iss_l13.pdf)\n",
    "\n",
    "[CMU Cross Validation](https://www.cs.cmu.edu/~schneide/tut5/node42.html)\n",
    "\n",
    "[3.1. Cross-validation: evaluating estimator performance](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "We have to be very careful about [overfitting](http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) on test set, so use the validation set.\n",
    "\n",
    "And when the validation set is small, when we tweak our model, the accuracy might change, but this could just be noise. And a useful rule of thumb is that, a change that affects 30 examples in your validation set, one way or another, is usually statistically significant and typically can be trusted. \n",
    "\n",
    "As 30 in 3000 is a 1% change, people tend to hold back 30,000 examples for validation, which makes the accuracy figures significant to the first decimal place and gives enough resolution to see small improvements. However, if my classes are not well-balanced and some classes are very rare compared to others, this heuristic is no longer good and more data would be needed. This means we need to hold back more than 30,000 data in training set, which is already a great amount, especially for some small training sets. In this case, cross-validation is one possible way to mitigate the issue but might take more time.\n",
    "\n",
    "Training logistic regression with gradient descent is a good trial as we are directly optimizing the error measure that we care about, and that is why in practice lots of ML research is about designing the right lost function to optimize. However, the biggest problem is that it is very difficult to scale. When larger dataset is fed in, the calculation would become humongous, and that is why we are using stochastic gradient descent.\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "If calculating loss function takes n floating point operations, computing its gradient takes about three times that compute. And for now, our loss function is already huge and depends on every element in the training set, so we are going to cheat.\n",
    "\n",
    "We will compute an estimate of the loss instead the loss itself. The estimate is simply computing the average loss for a very small random fraction of the training data, between 1 and 1000 training samples each time. (In fact, this estimate is so terrible that we have to use measures to make it less terrible.) This S.G.D is actually at the heart of deep learning because it can be efficiently scaled both with data and model size, which is exactly the big data and big models we are dealing with. But still, we need to solve a lot of issues coming with this rough estimation.\n",
    "\n",
    "Some measures we used to help SGD is listed below:\n",
    "1. Inputs\n",
    "    1. zero mean\n",
    "    2. equal and small variance\n",
    "2. Initial weights\n",
    "    1. random\n",
    "    2. zero mean\n",
    "    3. equal and small variance\n",
    "\n",
    "## Momentum\n",
    "\n",
    "We can refer from previous random steps because that on aggregate, those steps take us towards the minimum of loss. To keep a running average of the gradients and use this running average instead of the direction of the current batch of the data.\n",
    "\n",
    "1. For the traditional gradient descent, its gradient $\\Delta L$ and step direction $-\\alpha\\Delta L(w_i)$\n",
    "2. For the momentum technique, its gradient $\\Delta L + 0.9M$ and step direction $-\\alpha M(w_i)$\n",
    "\n",
    "This momentum technique works well and often leads to better convergence.\n",
    "\n",
    "## Learning Rate Decay\n",
    "\n",
    "As we are using SGD, we are taking smaller and noisier steps towards our objective. Besides all that still need to be researched on, making steps smaller and smaller as we train is usually beneficial. (There are many ways to determine this decaying learning rates, for example, we can lower the learning rate when the loss reach a plateau or simply use a exponential decay function to characterize it.)\n",
    "\n",
    "We might once tend to believe larger learning rates lead to faster learning; however, that is not true. High learning rates start the learning faster but then plateaus, while lower learning rates keep going and get better.\n",
    "\n",
    "So for SGD, we have many hyperparameters to play with:\n",
    "1. initial learning rate\n",
    "2. learning rate decay\n",
    "3. momentum\n",
    "4. batch size\n",
    "5. weight initialization\n",
    "\n",
    "We have to make these parameters right to make the neural network learn, but when things don't work, try to lower the learning rate first.\n",
    "\n",
    "There are similar measures like **AdaGrad**, which is a modififation of SGD which implicitly does momentum and learning rate decay. This will make tuning hyperparameters easier but the performance tends to be a little worse than precisely tuned SGD with momentum. At last, we need to remember we are still dealing with a linear model and no deep learning is into the play.\n",
    "\n",
    "# Mini-batch\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this sample, train_features Shape: (55000, 784) Type: float32 takes up the most memory, which is 172.48MB. However, larger datasets that we'll use in the future measured in gigabytes or more. \n",
    "\n",
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. \n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What does None do here?\n",
    "\n",
    "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples.\n",
    "\n",
    "Now let's see how to implement mini-batching. Implement the **batches function** to batch features and labels. The function should return each batch with a maximum size of batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's then use mini-batching to feed batches of MNIST features and labels into a linear model. Set the batch size and run the optimizer over all the batches with the batches function. The recommended batch size is 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.0854000002146\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times.\n",
    "# Epochs\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "The following TensorFlow code trains a model using 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 11.0     Valid Accuracy: 0.121\n",
      "Epoch: 1    - Cost: 10.2     Valid Accuracy: 0.129\n",
      "Epoch: 2    - Cost: 9.57     Valid Accuracy: 0.14 \n",
      "Epoch: 3    - Cost: 9.03     Valid Accuracy: 0.152\n",
      "Epoch: 4    - Cost: 8.56     Valid Accuracy: 0.169\n",
      "Epoch: 5    - Cost: 8.13     Valid Accuracy: 0.185\n",
      "Epoch: 6    - Cost: 7.75     Valid Accuracy: 0.207\n",
      "Epoch: 7    - Cost: 7.39     Valid Accuracy: 0.227\n",
      "Epoch: 8    - Cost: 7.07     Valid Accuracy: 0.244\n",
      "Epoch: 9    - Cost: 6.76     Valid Accuracy: 0.26 \n",
      "Epoch: 10   - Cost: 6.48     Valid Accuracy: 0.279\n",
      "Epoch: 11   - Cost: 6.22     Valid Accuracy: 0.297\n",
      "Epoch: 12   - Cost: 5.98     Valid Accuracy: 0.316\n",
      "Epoch: 13   - Cost: 5.75     Valid Accuracy: 0.333\n",
      "Epoch: 14   - Cost: 5.53     Valid Accuracy: 0.348\n",
      "Epoch: 15   - Cost: 5.33     Valid Accuracy: 0.364\n",
      "Epoch: 16   - Cost: 5.13     Valid Accuracy: 0.376\n",
      "Epoch: 17   - Cost: 4.95     Valid Accuracy: 0.393\n",
      "Epoch: 18   - Cost: 4.78     Valid Accuracy: 0.406\n",
      "Epoch: 19   - Cost: 4.62     Valid Accuracy: 0.42 \n",
      "Epoch: 20   - Cost: 4.47     Valid Accuracy: 0.434\n",
      "Epoch: 21   - Cost: 4.32     Valid Accuracy: 0.445\n",
      "Epoch: 22   - Cost: 4.19     Valid Accuracy: 0.462\n",
      "Epoch: 23   - Cost: 4.06     Valid Accuracy: 0.474\n",
      "Epoch: 24   - Cost: 3.95     Valid Accuracy: 0.483\n",
      "Epoch: 25   - Cost: 3.84     Valid Accuracy: 0.492\n",
      "Epoch: 26   - Cost: 3.73     Valid Accuracy: 0.503\n",
      "Epoch: 27   - Cost: 3.63     Valid Accuracy: 0.513\n",
      "Epoch: 28   - Cost: 3.54     Valid Accuracy: 0.521\n",
      "Epoch: 29   - Cost: 3.46     Valid Accuracy: 0.53 \n",
      "Epoch: 30   - Cost: 3.38     Valid Accuracy: 0.538\n",
      "Epoch: 31   - Cost: 3.3      Valid Accuracy: 0.545\n",
      "Epoch: 32   - Cost: 3.22     Valid Accuracy: 0.553\n",
      "Epoch: 33   - Cost: 3.16     Valid Accuracy: 0.56 \n",
      "Epoch: 34   - Cost: 3.09     Valid Accuracy: 0.565\n",
      "Epoch: 35   - Cost: 3.03     Valid Accuracy: 0.573\n",
      "Epoch: 36   - Cost: 2.97     Valid Accuracy: 0.581\n",
      "Epoch: 37   - Cost: 2.91     Valid Accuracy: 0.585\n",
      "Epoch: 38   - Cost: 2.86     Valid Accuracy: 0.592\n",
      "Epoch: 39   - Cost: 2.8      Valid Accuracy: 0.597\n",
      "Epoch: 40   - Cost: 2.75     Valid Accuracy: 0.601\n",
      "Epoch: 41   - Cost: 2.71     Valid Accuracy: 0.605\n",
      "Epoch: 42   - Cost: 2.66     Valid Accuracy: 0.61 \n",
      "Epoch: 43   - Cost: 2.62     Valid Accuracy: 0.614\n",
      "Epoch: 44   - Cost: 2.58     Valid Accuracy: 0.619\n",
      "Epoch: 45   - Cost: 2.54     Valid Accuracy: 0.623\n",
      "Epoch: 46   - Cost: 2.5      Valid Accuracy: 0.626\n",
      "Epoch: 47   - Cost: 2.46     Valid Accuracy: 0.63 \n",
      "Epoch: 48   - Cost: 2.43     Valid Accuracy: 0.633\n",
      "Epoch: 49   - Cost: 2.4      Valid Accuracy: 0.638\n",
      "Epoch: 50   - Cost: 2.36     Valid Accuracy: 0.641\n",
      "Epoch: 51   - Cost: 2.33     Valid Accuracy: 0.644\n",
      "Epoch: 52   - Cost: 2.3      Valid Accuracy: 0.647\n",
      "Epoch: 53   - Cost: 2.28     Valid Accuracy: 0.65 \n",
      "Epoch: 54   - Cost: 2.25     Valid Accuracy: 0.653\n",
      "Epoch: 55   - Cost: 2.22     Valid Accuracy: 0.655\n",
      "Epoch: 56   - Cost: 2.2      Valid Accuracy: 0.659\n",
      "Epoch: 57   - Cost: 2.17     Valid Accuracy: 0.66 \n",
      "Epoch: 58   - Cost: 2.15     Valid Accuracy: 0.663\n",
      "Epoch: 59   - Cost: 2.13     Valid Accuracy: 0.666\n",
      "Epoch: 60   - Cost: 2.1      Valid Accuracy: 0.668\n",
      "Epoch: 61   - Cost: 2.08     Valid Accuracy: 0.671\n",
      "Epoch: 62   - Cost: 2.06     Valid Accuracy: 0.674\n",
      "Epoch: 63   - Cost: 2.04     Valid Accuracy: 0.676\n",
      "Epoch: 64   - Cost: 2.02     Valid Accuracy: 0.679\n",
      "Epoch: 65   - Cost: 2.01     Valid Accuracy: 0.681\n",
      "Epoch: 66   - Cost: 1.99     Valid Accuracy: 0.682\n",
      "Epoch: 67   - Cost: 1.97     Valid Accuracy: 0.683\n",
      "Epoch: 68   - Cost: 1.95     Valid Accuracy: 0.685\n",
      "Epoch: 69   - Cost: 1.94     Valid Accuracy: 0.686\n",
      "Epoch: 70   - Cost: 1.92     Valid Accuracy: 0.689\n",
      "Epoch: 71   - Cost: 1.9      Valid Accuracy: 0.69 \n",
      "Epoch: 72   - Cost: 1.89     Valid Accuracy: 0.692\n",
      "Epoch: 73   - Cost: 1.87     Valid Accuracy: 0.695\n",
      "Epoch: 74   - Cost: 1.86     Valid Accuracy: 0.698\n",
      "Epoch: 75   - Cost: 1.85     Valid Accuracy: 0.699\n",
      "Epoch: 76   - Cost: 1.83     Valid Accuracy: 0.703\n",
      "Epoch: 77   - Cost: 1.82     Valid Accuracy: 0.705\n",
      "Epoch: 78   - Cost: 1.81     Valid Accuracy: 0.707\n",
      "Epoch: 79   - Cost: 1.79     Valid Accuracy: 0.708\n",
      "Epoch: 80   - Cost: 1.78     Valid Accuracy: 0.709\n",
      "Epoch: 81   - Cost: 1.77     Valid Accuracy: 0.711\n",
      "Epoch: 82   - Cost: 1.76     Valid Accuracy: 0.712\n",
      "Epoch: 83   - Cost: 1.74     Valid Accuracy: 0.714\n",
      "Epoch: 84   - Cost: 1.73     Valid Accuracy: 0.715\n",
      "Epoch: 85   - Cost: 1.72     Valid Accuracy: 0.716\n",
      "Epoch: 86   - Cost: 1.71     Valid Accuracy: 0.719\n",
      "Epoch: 87   - Cost: 1.7      Valid Accuracy: 0.72 \n",
      "Epoch: 88   - Cost: 1.69     Valid Accuracy: 0.722\n",
      "Epoch: 89   - Cost: 1.68     Valid Accuracy: 0.723\n",
      "Epoch: 90   - Cost: 1.67     Valid Accuracy: 0.725\n",
      "Epoch: 91   - Cost: 1.66     Valid Accuracy: 0.726\n",
      "Epoch: 92   - Cost: 1.65     Valid Accuracy: 0.728\n",
      "Epoch: 93   - Cost: 1.64     Valid Accuracy: 0.73 \n",
      "Epoch: 94   - Cost: 1.63     Valid Accuracy: 0.732\n",
      "Epoch: 95   - Cost: 1.62     Valid Accuracy: 0.733\n",
      "Epoch: 96   - Cost: 1.61     Valid Accuracy: 0.734\n",
      "Epoch: 97   - Cost: 1.6      Valid Accuracy: 0.734\n",
      "Epoch: 98   - Cost: 1.59     Valid Accuracy: 0.736\n",
      "Epoch: 99   - Cost: 1.58     Valid Accuracy: 0.737\n",
      "Test Accuracy: 0.732699990273\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The accuracy only reached 0.73, but that could be because the learning rate was too high. Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy. And in the upcoming TensorFLow Lab, we are to choose own learning rate, epoch count, and batch size to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
